OpenEmbedded/Yocto BSP layer for NVIDIA Tegra X1/X2/AGX/K1
==========================================================

Boards supported:
* Jetson-TK1 development kit (Linux4Tegra R21.7)
* Jetson-TX1 development kit (Linux4Tegra R32.1, JetPack 4.2 [see note below])
* Jetson-TX2 development kit (Linux4Tegra R32.1, JetPack 4.2)
* Jetson AGX Xavier development kit (Linux4Tegra R32.1, JetPack 4.2)
* Jetson Nano development kit (Linux4Tegra R32.1, JetPack 4.2)

Also supported:
* Jetson-TX2i module (Linux4Tegra R32.1, JetPack 4.2)


This layer depends on:
URI: git://git.openembedded.org/openembedded-core
branch: warrior
LAYERSERIES_COMPAT: warrior


PLEASE NOTE
-----------

* Starting with JetPack 4.2, packages outside the L4T BSP can
  only be downloaded with an NVIDIA Developer Network login.
  So to use CUDA 10, cuDNN, and any other packages that require
  a Devnet login, you **must** create a Devnet account and
  download the JetPack packages you need for your builds using
  NVIDIA SDK Manager.

  You must then set the variable NVIDIA_DEVNET_MIRROR to
  "file://path/to/the/downloads" in your build configuration
  (e.g., local.conf) to make them available to your bitbake
  builds.

  For Jetson-TX2 builds, you must also **manually** download
  the [Jetson Platform Fuse Burning and Secure Boot Documentation
  and Tools](http://developer.nvidia.com/embedded/dlc/jetson-fuse-burning-secure-boot-32-1)
  package, as this package is not distributed through the
  NVIDIA SDK Manager. If NVIDIA_DEVNET_MIRROR is not set
  in your build configuration, you can omit this step
  and image signing support will be excluded.

* The SDK Manager downloads a different package of CUDA host-side
  tools depending on whether you are running Ubuntu 16.04
  or 18.04. If you downloaded the Ubuntu 16.04 package, you
  should add

      CUDA_BINARIES_NATIVE = "cuda-binaries-ubuntu1604-native"

  to your build configuration so the CUDA recipes can find
  them. Otherwise, the recipes will default to looking for
  the Ubuntu 18.04 package.

* The tensorrt 5.1.6 packages for Xavier are different from
  those for TX1/TX2, even though the deb files have the same
  name.  If you need to build for Xavier and another platform
  and include tensorrt 5.1.6, create a subdirectory called
  "P2888" under your NVIDIA_DEVNET_MIRROR directory, and copy
  the Xavier tensorrt packages there. The non-Xavier copies
  should go in the NVIDIA_DEVNET_MIRROR top level.

* CUDA 10 supports up through gcc 7 only, and some NVIDIA-provided
  binary libraries appear to be compiled with g++ 7 and cause linker
  failures when building applications with g++ 6, so **only** gcc 7
  should be used if you intend to use CUDA. See the following wiki
  pages for instructions on including gcc 7 in your builds:

  (https://github.com/madisongh/meta-tegra/wiki/Using-gcc7-from-the-contrib-layer)
  (https://github.com/madisongh/meta-tegra/wiki/Using-linaro-gcc7-for-CUDA-support)


Contributing
------------

Please use GitHub (https://github.com/madisongh/meta-tegra) to submit
issues or pull requests, or add to the documentation on the wiki.
Contributions are welcome!
